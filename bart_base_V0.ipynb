{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462dcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f70445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vazgen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2e88e",
   "metadata": {},
   "source": [
    "Loading the data and pretrained model from the Hugging Face repository "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c46806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/Users/vazgen/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3258159167534e9f967bfe7fba608827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ae6afb765781>:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('rouge')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#data = load_dataset('samsum')\n",
    "#data.save_to_disk('/content/samsum')\n",
    "data = load_dataset(\"samsum\")\n",
    "metric = load_metric('rouge')\n",
    "model_checkpoints = 'facebook/bart-large-xsum'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad9e45a",
   "metadata": {},
   "source": [
    "Defining model varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11d9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_input = 512\n",
    "max_target = 128\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoints)\n",
    "device = torch.device('mps')\n",
    "\n",
    "# tokenizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f3a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING FUCTION FOR PROCESSING DATA\n",
    "\n",
    "def preprocess_data(data_to_process):\n",
    "    #get the dialogue text\n",
    "    inputs = [dialogue for dialogue in data_to_process['dialogue']]\n",
    "    #tokenize text\n",
    "    model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
    "  \n",
    "    #tokenize labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(data_to_process['summary'], max_length=max_target, padding='max_length', truncation=True)\n",
    "      \n",
    "    model_inputs['labels'] = targets['input_ids']\n",
    "    #reuturns input_ids, attention_masks, labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da29b370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/vazgen/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-274c4ece19d71c23.arrow\n",
      "Loading cached processed dataset at /Users/vazgen/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-031e961de5c55c00.arrow\n",
      "Loading cached processed dataset at /Users/vazgen/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-4e9917fd7977320c.arrow\n"
     ]
    }
   ],
   "source": [
    "#tokenizing data\n",
    "tokenize_data = data.map(preprocess_data, batched = True, remove_columns=['id', 'dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c80275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf77d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0451f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/vazgen/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-9567d0d2232a6b7a.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/vazgen/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-9a8841ff94abe0e0.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/vazgen/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-22334d949f69a92d.arrow\n"
     ]
    }
   ],
   "source": [
    "#splitting data for illustrative purpose\n",
    "\n",
    "#sample the data\n",
    "train_sample = tokenize_data['train'].shuffle(seed=123).select(range(1000))\n",
    "validation_sample = tokenize_data['validation'].shuffle(seed=123).select(range(200))\n",
    "test_sample = tokenize_data['test'].shuffle(seed=123).select(range(200))\n",
    "\n",
    "tokenize_data['train'] = train_sample\n",
    "tokenize_data['validation'] = validation_sample\n",
    "tokenize_data['test'] = test_sample\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa828d1",
   "metadata": {},
   "source": [
    "# Initializing Model with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95aaad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e030615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)\n",
    "#collator to create batches. It preprocess data with the given tokenizer\n",
    "collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f713016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################\n",
    "# metrics\n",
    "# compute rouge for evaluation \n",
    "#####################\n",
    "\n",
    "def compute_rouge(pred):\n",
    "    predictions, labels = pred\n",
    "    #decode the predictions\n",
    "    decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    #decode labels\n",
    "    decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  \n",
    "    #compute results\n",
    "    res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n",
    "    #get %\n",
    "    res = {key: value.mid.fmeasure * 100 for key, value in res.items()}\n",
    "  \n",
    "    pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    res['gen_len'] = np.mean(pred_lens)\n",
    "  \n",
    "    return {k: round(v, 4) for k, v in res.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2e3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = transformers.Seq2SeqTrainingArguments(\n",
    "    'conversation-summ',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size= 1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=1,\n",
    "#     fp16=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e45746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset=tokenize_data['train'],\n",
    "    eval_dataset=tokenize_data['validation'],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_rouge\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173838de",
   "metadata": {},
   "source": [
    "Fitting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0607a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vazgen/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 3:23:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>0.358164</td>\n",
       "      <td>49.186900</td>\n",
       "      <td>24.529500</td>\n",
       "      <td>39.786200</td>\n",
       "      <td>39.768500</td>\n",
       "      <td>23.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.367485</td>\n",
       "      <td>50.832700</td>\n",
       "      <td>24.929000</td>\n",
       "      <td>40.741500</td>\n",
       "      <td>40.661100</td>\n",
       "      <td>26.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.415083</td>\n",
       "      <td>51.023700</td>\n",
       "      <td>24.166800</td>\n",
       "      <td>39.646000</td>\n",
       "      <td>39.612500</td>\n",
       "      <td>29.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to conversation-summ/checkpoint-500\n",
      "Configuration saved in conversation-summ/checkpoint-500/config.json\n",
      "Model weights saved in conversation-summ/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in conversation-summ/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in conversation-summ/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [conversation-summ/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to conversation-summ/checkpoint-1000\n",
      "Configuration saved in conversation-summ/checkpoint-1000/config.json\n",
      "Model weights saved in conversation-summ/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in conversation-summ/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in conversation-summ/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [conversation-summ/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to conversation-summ/checkpoint-1500\n",
      "Configuration saved in conversation-summ/checkpoint-1500/config.json\n",
      "Model weights saved in conversation-summ/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in conversation-summ/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in conversation-summ/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [conversation-summ/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.2978282725016276, metrics={'train_runtime': 12233.3307, 'train_samples_per_second': 0.245, 'train_steps_per_second': 0.123, 'total_flos': 3250656903168000.0, 'train_loss': 0.2978282725016276, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "762c1806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 14:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41508251428604126,\n",
       " 'eval_rouge1': 51.0237,\n",
       " 'eval_rouge2': 24.1668,\n",
       " 'eval_rougeL': 39.646,\n",
       " 'eval_rougeLsum': 39.6125,\n",
       " 'eval_gen_len': 29.625,\n",
       " 'eval_runtime': 848.0805,\n",
       " 'eval_samples_per_second': 0.236,\n",
       " 'eval_steps_per_second': 0.236,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a940d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wanda: Let's make a party!\r\n",
      "Gina: Why?\r\n",
      "Wanda: beacuse. I want some fun!\r\n",
      "Gina: ok, what do u need?\r\n",
      "Wanda: 1st I need too make a list\r\n",
      "Gina: noted and then?\r\n",
      "Wanda: well, could u take yours father car and go do groceries with me?\r\n",
      "Gina: don't know if he'll agree\r\n",
      "Wanda: I know, but u can ask :)\r\n",
      "Gina: I'll try but theres no promisess\r\n",
      "Wanda: I know, u r the best!\r\n",
      "Gina: When u wanna go\r\n",
      "Wanda: Friday?\r\n",
      "Gina: ok, I'll ask\n"
     ]
    }
   ],
   "source": [
    "print(data['test'][10]['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78b1fca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wanda wants to have a party. Wanda and Gina will go shopping on Friday. Gina will take her father's car and go shopping with Wanda.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "conversation = \"\"\"Wanda: Let's make a party!\n",
    "Gina: Why?\n",
    "Wanda: beacuse. I want some fun!\n",
    "Gina: ok, what do u need?\n",
    "Wanda: 1st I need too make a list\n",
    "Gina: noted and then?\n",
    "Wanda: well, could u take yours father car and go do groceries with me?\n",
    "Gina: don't know if he'll agree\n",
    "Wanda: I know, but u can ask :)\n",
    "Gina: I'll try but theres no promisess\n",
    "Wanda: I know, u r the best!\n",
    "Gina: When u wanna go\n",
    "Wanda: Friday?\n",
    "Gina: ok, I'll ask\n",
    "\"\"\"\n",
    "#tokenize the conversation\n",
    "model_inputs = tokenizer(conversation,  max_length=max_input, padding='max_length', truncation=True)\n",
    "#make prediction\n",
    "raw_pred, _, _ = trainer.predict([model_inputs])\n",
    "\n",
    "result = tokenizer.decode(raw_pred[0]).replace(\"</s>\",\"\").replace(\"<pad>\",\"\")\n",
    "#decode the output\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814165a",
   "metadata": {},
   "source": [
    "Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18aa2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = transformers.Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"mt5-small-finetune-sumsum\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "#     logging_steps=logging_steps,\n",
    "#     push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "829abb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /Users/vazgen/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/spiece.model from cache at /Users/vazgen/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/special_tokens_map.json from cache at /Users/vazgen/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/tokenizer_config.json from cache at /Users/vazgen/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32\n",
      "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /Users/vazgen/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /Users/vazgen/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "158100c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2842fa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Paligonshik/mt5-small-finetune-sumsum/resolve/main/config.json from cache at /Users/vazgen/.cache/huggingface/transformers/9633a5c3ab445d813c376ecfdaa6d0d708b24e7cf9efc04acda49bc1d409dcc4.befe39e9588513959b94c3916f8f56144e1d9cf5ec098657fc5af1fde9d5439c\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"Paligonshik/mt5-small-finetune-sumsum\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Paligonshik/mt5-small-finetune-sumsum/resolve/main/pytorch_model.bin from cache at /Users/vazgen/.cache/huggingface/transformers/58e339311cae19a19a9e05b053619375a837d5112245ec5b0e0e4d613e1bc8d1.24a5aeeac4b9f274feff4e164f25178562e32d190ded314fb7b8ac9a09a696ad\n",
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at Paligonshik/mt5-small-finetune-sumsum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model2 = AutoModelForSeq2SeqLM.from_pretrained('Paligonshik/mt5-small-finetune-sumsum')\n",
    "trainer2 = Seq2SeqTrainer(\n",
    "    model2,\n",
    "    args,\n",
    "    train_dataset=tokenize_data[\"train\"],\n",
    "    eval_dataset=tokenize_data[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5812bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0ebed49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13680771',\n",
       " 'dialogue': \"Wanda: Let's make a party!\\r\\nGina: Why?\\r\\nWanda: beacuse. I want some fun!\\r\\nGina: ok, what do u need?\\r\\nWanda: 1st I need too make a list\\r\\nGina: noted and then?\\r\\nWanda: well, could u take yours father car and go do groceries with me?\\r\\nGina: don't know if he'll agree\\r\\nWanda: I know, but u can ask :)\\r\\nGina: I'll try but theres no promisess\\r\\nWanda: I know, u r the best!\\r\\nGina: When u wanna go\\r\\nWanda: Friday?\\r\\nGina: ok, I'll ask\",\n",
       " 'summary': \"Wanda wants to throw a party. She asks Gina to borrow her father's car and go do groceries together. They set the date for Friday. \"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a911acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"dialogue\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cee4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = preprocess_function(data['test'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c25f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16d30c73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer2.predict([test_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c715030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Wanda wants to throw a party. She asks Gina to borrow her father's car and go do groceries</s>\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = trainer2.predict([test_sample])\n",
    "tokenizer.decode(pred[1][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
